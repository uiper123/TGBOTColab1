# GPU Оптимизации для максимального использования памяти

## Внесенные изменения:

### 1. video_processor.py
- ✅ Добавлено ПОЛНОЕ GPU ускорение с `-hwaccel cuda` и `-hwaccel_output_format cuda`
- ✅ Увеличены параметры GPU для использования больше VRAM:
  - `surfaces: 32` - больше поверхностей GPU
  - `rc-lookahead: 32` - увеличенный lookahead
  - `bufsize: 16M` - больший буфер
- ✅ Добавлен специальный метод для AV1 с GPU декодированием
- ✅ Улучшена проверка GPU с детальной диагностикой

### 2. video_editor.py  
- ✅ Добавлено GPU декодирование входного видео с `hwaccel='cuda'`
- ✅ Увеличены параметры NVENC для максимального использования GPU:
  - `preset='p2'` - быстрый пресет
  - `cq=20` - высокое качество
  - `b:v='8M'` - увеличенный битрейт
  - `maxrate='12M'` - увеличенный максимальный битрейт
  - `bufsize='16M'` - больший буфер (больше VRAM)
  - `surfaces='32'` - больше поверхностей GPU
  - `rc-lookahead='32'` - больше GPU вычислений

### 3. Дополнительные оптимизации
- ✅ Принудительное использование первого GPU с `gpu=0`
- ✅ Минимальная задержка с `delay=0`
- ✅ Variable bitrate для оптимального качества

### 4. Увеличенный параллелизм для GPU
- ✅ Увеличено количество параллельных процессов нарезки чанков: 6 для GPU (было 3)
- ✅ Увеличено количество параллельных процессов создания клипов: 8 для GPU (было 4)
- ✅ Автоматическое определение оптимального параллелизма в зависимости от GPU/CPU

## Результат:
Теперь система должна использовать значительно больше GPU памяти (до 10-12 ГБ из 15 ГБ Tesla T4) за счет:
1. **GPU декодирования** входного видео (`-hwaccel cuda`)
2. **Увеличенных буферов** и поверхностей (`bufsize=16M`, `surfaces=32`)
3. **Более агрессивных настроек** NVENC (`preset=p2`, `rc-lookahead=32`)
4. **Увеличенного параллелизма** (6 чанков + 8 клипов одновременно)
5. **Полного GPU пайплайна** (декодирование → обработка → кодирование)

## Тестирование:
1. Запустите `python test_gpu_usage.py` для тестирования
2. В другом терминале: `watch -n 1 nvidia-smi`
3. Вы должны увидеть использование 10-12 ГБ VRAM вместо 2 ГБ

## Мониторинг:
```bash
# Постоянный мониторинг GPU
watch -n 1 nvidia-smi

# Детальная информация о процессах
nvidia-smi pmon -i 0 -s um
```